# -*- coding: utf-8 -*-
"""ETL - NOLA Capstone

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1aiMQilAz3DBTIX6ITzmx_6NxF4vY0fxu

##I. Load and Transform 311 Data 2012 - Present
"""

#Import Pandas
import pandas as pd

#Import dataqfrom city data website
df1 = pd.read_csv('https://data.nola.gov/resource/2jgv-pqrq.csv')

#cols = df1["requesttype"].to_list()
#print(cols) #To find the correct property to focus on

"""The dataset includes many different 311 calls, however we only care about the ones related to blight, which is under Property Maintenance"""

df_blight = df1[df1["request_type"] == "Property Maintenance"]
cols = df_blight["status"].to_list()
#print(cols) #seeing what status we would like to keep

"""We will now drop any that are resolved, since we will not need properties that have been resolved. We will also drop any unimportant information, and set the index to the address"""

df_blight = df_blight.drop(df_blight[df_blight["status"] == "Resolved"].index)

df_blight.drop('rowid', axis=1, inplace = True)
df_blight.drop('final_x', axis=1, inplace = True)
df_blight.drop('final_y', axis=1, inplace = True)

# Adding a new column 'Data Source' for df_blight
df_blight['Data Source'] = '311 Calls'

"""We first checked to see if there are any values for contractor, which there are none, so we will drop this."""

df_blight["contractor"].unique()
df_blight["contractor_action"].unique()
#Both do not have any values.
df_blight.drop('contractor', axis=1, inplace = True)
df_blight.drop('contractor_action', axis=1, inplace = True)
df_blight.drop('service_request', axis=1, inplace = True)
df_blight.drop('request_type', axis=1, inplace = True)
df_blight.drop('request_reason', axis=1, inplace = True)
df_blight.drop('date_modified', axis=1, inplace = True)
df_blight.drop('case_close_date', axis=1, inplace = True)
df_blight.drop('responsible_agency', axis=1, inplace = True)
df_blight.drop('address_councildis', axis=1, inplace = True)
df_blight.drop('status', axis=1, inplace = True)

df_blight

"""##II. Load and Transform Inspection Data"""

df2 = pd.read_csv('https://data.nola.gov/resource/d4d6-yab5.csv')
display(df2.dtypes) #looking to see what all of the columns are, and to drop the unnecessary ones.

"""We will once again drop unimportant data, and only those under the status "Open". We will then change the index to the address and making the address the index. We will change the name of the column to final_address, which also lines up with the first dataset.

One problem when combining the datasets is that there is important information that is included in both, however the other dataset does not have the information. Discussion of what to do about that is ongoing.
"""

df_blight2 = df2[df2["o_c"] == "Open"]
df_blight2 = df_blight2.rename(columns = {"location": "final_address"})
#df_blight2 = df_blight2.set_index("final_address")
df_blight2.drop('prevhearingdate', axis=1, inplace = True)
df_blight2.drop('prevhearingresult', axis=1, inplace = True)
df_blight2.drop('initinspection', axis=1, inplace = True)
df_blight2.drop('statdate', axis=1, inplace = True)
df_blight2.drop('nexthearingdate', axis=1, inplace = True)
df_blight2.drop('the_geom', axis=1, inplace = True)
df_blight2.drop('caseid', axis=1, inplace = True)
df_blight2.drop('caseno', axis=1, inplace = True)
df_blight2.drop('geopin', axis=1, inplace = True)
df_blight2.drop('initinspresult', axis=1, inplace = True)
df_blight2.drop('keystatus', axis=1, inplace = True)
df_blight2.drop('lastpermit', axis=1, inplace = True)
df_blight2.drop('permitstatus', axis=1, inplace = True)
df_blight2.drop('permittype', axis=1, inplace = True)
df_blight2.drop('zipcode', axis=1, inplace = True)
df_blight2.drop('permitfiling', axis=1, inplace = True)
df_blight2['Data Source'] = 'Code Enforcement'

print(df_blight2.columns)

df_blight2

from geopy.geocoders import Nominatim
import pandas as pd
import numpy as np  # for np.nan

geolocator = Nominatim(user_agent="my_geocoder")
latitude = []
longitude = []

# Geographic bounds for New Orleans
LAT_MIN, LAT_MAX = 29.9011, 30.0723
LON_MIN, LON_MAX = -90.1363, -89.911

for address in df_blight2['final_address']:
    try:
        loc = geolocator.geocode(f"{address}, New Orleans, LA", timeout=10) #Added city and state specifications
        if loc:
            lat, lon = loc.latitude, loc.longitude
            # Check if the coordinates are within the defined limits
            if LAT_MIN <= lat <= LAT_MAX and LON_MIN <= lon <= LON_MAX:
                latitude.append(lat)
                longitude.append(lon)
            else:
                latitude.append(np.nan)
                longitude.append(np.nan)
        else:
            latitude.append(np.nan)
            longitude.append(np.nan)
    except Exception as e:
        print(f"Error geocoding {address}: {e}")
        latitude.append(np.nan)
        longitude.append(np.nan)

df_blight2['latitude'] = latitude
df_blight2['longitude'] = longitude

print(latitude)

df_blight_copies = df_blight.merge(df_blight2, how='left', on="final_address", copy=None)

df_blight3 = pd.concat([df_blight, df_blight2], copy = False)

df_blight3

# Count the number of rows where both latitude and longitude are NaN
nan_count = df_blight3[df_blight3['latitude'].isna() & df_blight3['longitude'].isna()].shape[0]

# Print the result
print("Number of rows with NaN for both latitude and longitude:", nan_count)

# Remove rows where both latitude and longitude are NaN
df_blight3_cleaned = df_blight3.dropna(subset=['latitude', 'longitude'], how='all')
# Check remaining NaN counts to confirm removal
remaining_nans = df_blight3_cleaned[df_blight3_cleaned['latitude'].isna() & df_blight3_cleaned['longitude'].isna()].shape[0]
print("Remaining rows with NaN for both latitude and longitude:", remaining_nans)

df_blight2['address'] = df_blight2['geoaddress'] + ", "

df_blight2.columns

"""Zipcode converter (using longitude and latitude)"""

import pandas as pd
import numpy as np
from geopy.geocoders import Nominatim
from geopy.extra.rate_limiter import RateLimiter

geolocator = Nominatim(user_agent="my_geocoder")

# Function to perform reverse geocoding with a timeout and extract the zip code
def extract_zip_code(lat, lon):
    if pd.isna(lat) or pd.isna(lon):
        return np.nan  # Return NaN for missing or invalid coordinates

    try:
        location = geolocator.reverse((lat, lon), exactly_one=True, timeout=10)
        if location:
            # Extract zip code from the location's address component
            return location.raw.get('address', {}).get('postcode')
        return np.nan
    except Exception as e:
        print(f"Error reverse geocoding {lat}, {lon}: {e}")
        return np.nan

if 'latitude' in df_blight3.columns and 'longitude' in df_blight3.columns:
    df_blight3['zip_code'] = df_blight3.apply(
        lambda row: extract_zip_code(row['latitude'], row['longitude']), axis=1
    )

print(df_blight3[['final_address', 'latitude', 'longitude', 'zip_code']].head())

# Check for missing values in zip codes
missing_zip_codes = df_blight3['zip_code'].isna().sum()
print(f"Missing zip codes: {missing_zip_codes}")

# Get a count of unique zip codes
unique_zip_codes = df_blight3['zip_code'].nunique()
print(f"Unique zip codes: {unique_zip_codes}")

# Display unique zip codes to understand the data better
print(df_blight3['zip_code'].value_counts())

"""##III. USPS Data"""

!pip install dbfread

from google.colab import files

uploaded = files.upload()

from dbfread import DBF
import pandas as pd


dbf = DBF('usps_vac_122023_tractsum_2kx.dbf')
dfusps = pd.DataFrame(iter(dbf))
# Save to CSV
dfusps.to_csv('usps_vac.csv', index=False)

dfusps

dfusps.info()

"""GeoID to address converter"""

'''
import geopandas as gpd

# Load the shapefile that corresponds to the GEOID level (tract, block, etc.)
gdf = gpd.read_file('path_to_shapefile.shp')

# Calculate centroids of each geometry in the GeoDataFrame
gdf['centroid'] = gdf.geometry.centroid
gdf['latitude'] = gdf.centroid.y
gdf['longitude'] = gdf.centroid.x
'''

"""##IV. Database Connection"""

#Download df to csv for dasboard
df_blight.to_csv()

!pip install mysql-connector-python

'''
import mysql.connector

host = "sql5.freesqldatabase.com"
user = "sql5680691"
password = "g4fgFKv83C"
database = "sql5680691"
port = 3306

connection = mysql.connector.connect(
    host=host,
    user=user,
    password=password,
    database=database,
    port=port
)

mycursor = connection.cursor()

def handle_nan(value):
    return value if pd.notna(value) else None

# Insert data into df_blight
for index, row in df_blight.iterrows():
    mycursor.execute("""
        INSERT INTO df_blight (`request_status`, `final_address`, `longitude`, `latitude`, `geocoded_column`, `Data Source`)
        VALUES (%s, %s, %s, %s, %s, %s)
    """, (
        handle_nan(row['request_status']),
        handle_nan(row['final_address']),
        handle_nan(row['longitude']),
        handle_nan(row['latitude']),
        handle_nan(row['geocoded_column']),
        handle_nan(row['Data Source'])
    ))

# Insert data into df_blight2
for index, row in df_blight2.iterrows():
    mycursor.execute("""
        INSERT INTO df_blight2 (`objectid`, `casefiled`, `geoaddress`, `o_c`, `final_address`, `Data Source`)
        VALUES (%s, %s, %s, %s, %s, %s)
    """, (
        handle_nan(row['objectid']),
        handle_nan(row['casefiled']),
        handle_nan(row['geoaddress']),
        handle_nan(row['o_c']),
        handle_nan(row.get('final_address')),
        handle_nan(row['Data Source'])
    ))

# Insert data into df_blight3
for index, row in df_blight3.iterrows():
    mycursor.execute("""
        INSERT INTO df_blight3 (`request_status`, `final_address`, `longitude`, `latitude`, `geocoded_column`, `objectid`, `casefiled`, `geoaddress`, `o_c`, `Data Source`)
        VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s)
    """, (
        handle_nan(row['request_status']),
        handle_nan(row['final_address']),
        handle_nan(row['longitude']),
        handle_nan(row['latitude']),
        handle_nan(row['geocoded_column']),
        handle_nan(row.get('objectid')),
        handle_nan(row.get('casefiled')),
        handle_nan(row.get('geoaddress')),
        handle_nan(row.get('o_c')),
        handle_nan(row['Data Source'])
    ))
connection.commit()

mycursor.close()
connection.close()
'''

'''
import mysql.connector
import pandas as pd

# Connect to the database
conn = mysql.connector.connect(
    host="sql5.freesqldatabase.com",
    user="sql5680691",
    password="g4fgFKv83C",
    database="sql5680691",
    port=3306
)
cursor = conn.cursor()

# Helper function to handle NaN values
def handle_nan(value):
    return None if pd.isna(value) else value

# Insert data into the new table
for index, row in df_blight3.iterrows():
    sql =
    values = (
        handle_nan(row['final_address']),
        handle_nan(row['request_status']),
        handle_nan(row['objectid']),
        handle_nan(row['casefiled']),
        handle_nan(row['o_c']),
        handle_nan(row['latitude']),
        handle_nan(row['longitude']),
        handle_nan(row['Data Source']),
        handle_nan(row['Zip Code'])
    )
    cursor.execute(sql, values)

conn.commit()
cursor.close()
conn.close()
'''

'''
import mysql.connector

conn = mysql.connector.connect(
    host="sql5.freesqldatabase.com",
    user="sql5680691",
    password="g4fgFKv83C",
    database="sql5680691",
    port=3306
)
cursor = conn.cursor()
sql_delete = "DELETE FROM blightdata WHERE latitude IS NULL OR longitude IS NULL"
cursor.execute(sql_delete)

conn.commit()  # Commit the changes
print(f"Deleted {cursor.rowcount} rows")  # Output how many rows were deleted

cursor.close()
conn.close()
'''

import mysql.connector
import pandas as pd

def handle_nan(value):
    return None if pd.isna(value) else value

try:
    # Connect to the database
    conn = mysql.connector.connect(
        host="sql5.freesqldatabase.com",
        user="sql5680691",
        password="g4fgFKv83C",
        database="sql5680691",
        port=3306
    )
    cursor = conn.cursor()

    # SQL command to insert data into the 'blightdata' table
    sql_insert = """
    INSERT INTO blightdata (final_address, request_status, objectid, casefiled, o_c, latitude, longitude, data_source, zipcode, state)
    VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s)
    ON DUPLICATE KEY UPDATE
    request_status=VALUES(request_status),
    objectid=VALUES(objectid),
    casefiled=VALUES(casefiled),
    o_c=VALUES(o_c),
    latitude=VALUES(latitude),
    longitude=VALUES(longitude),
    data_source=VALUES(data_source),
    zipcode=VALUES(zipcode);
    """

    # Insert data into the new table
    for index, row in df_blight3.iterrows():
        values = (
            handle_nan(row['final_address']),
            handle_nan(row['request_status']),
            handle_nan(row['objectid']),
            handle_nan(row['casefiled']),
            handle_nan(row['o_c']),
            handle_nan(row['latitude']),
            handle_nan(row['longitude']),
            handle_nan(row['Data Source']),
            handle_nan(row['zip_code'])
            handle_nan(row['stage'])
        )
        cursor.execute(sql_insert, values)

    # SQL command to delete rows where latitude or longitude are NULL
    sql_delete = "DELETE FROM blightdata WHERE latitude IS NULL OR longitude IS NULL"
    cursor.execute(sql_delete)
    conn.commit()  # Commit the changes

    # Output how many rows were inserted and deleted
    print(f"Inserted/Updated rows and deleted {cursor.rowcount} rows where latitude or longitude were NULL.")

except mysql.connector.Error as e:
    print(f"Error: {e}")

finally:
    if conn.is_connected():
        cursor.close()
        conn.close()
        print("MySQL connection is closed")

"""USPS data to Database Insertion"""

import mysql.connector
import pandas as pd
import numpy as np

def handle_nan(value):
    if pd.isna(value):
        return None
    elif isinstance(value, np.number):
        return float(value) if isinstance(value, np.floating) else int(value)
    return value

# Establish database connection
conn = mysql.connector.connect(
    host="sql5.freesqldatabase.com",
    user="sql5680691",
    password="g4fgFKv83C",
    database="sql5680691",
    port=3306
)
cursor = conn.cursor()

# Create a SQL insertion template
columns = ', '.join(dfusps.columns)
placeholders = ', '.join(['%s'] * len(dfusps.columns))
sql = f"INSERT INTO dfusps ({columns}) VALUES ({placeholders})"

# Insert data
try:
    for index, row in dfusps.iterrows():
        values = tuple(handle_nan(row[col]) for col in dfusps.columns)
        cursor.execute(sql, values)
    conn.commit()
    print(f"Successfully inserted {cursor.rowcount} rows.")
except mysql.connector.Error as e:
    print(f"Error: {e}")

# Close connection
cursor.close()
conn.close()